\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{color}
\usepackage{amsmath, amssymb}
% \usepackage{hyperref}

\textheight=8.85in

\pagestyle{myheadings}

\setlength{\tabcolsep}{0in}
\begin{document}

\thispagestyle {empty}

\newcommand{\lsp}[1]{\large\renewcommand{\baselinestretch}{#1}\normalsize}
\newcommand{\hsp}{\hspace{.2in}}
\newcommand{\comment}[1]{}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{problem}{Problem}[section]

\newcommand{\R}{{\rm\hbox{I\kern-.15em R}}}
\newcommand{\IR}{{\rm\hbox{I\kern-.15em R}}}
\newcommand{\IN}{{\rm\hbox{I\kern-.15em N}}}
\newcommand{\IZ}{{\sf\hbox{Z\kern-.40em Z}}}
\newcommand{\IS}{{\rm\hbox{S\kern-.45em S}}}
\newcommand{\Real}{I\!\!R}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bR}{\mathbf{R}}

\newcommand{\linesep}{\vspace{.2cm}\hrule\vspace{0.2cm}}
\newcommand{\categorysep}{\vspace{0.5cm}}
\newcommand{\entrysep}{\vspace{0cm}}

\newcommand{\category}[1]{\categorysep
                  \noindent {\bf \large #1}
              \linesep}

\pagestyle{empty}

\begin{center}
{\large \textbf{CSE 847 (Spring 2021): Machine Learning--- Homework 3}} \\
 Instructor: Jiayu Zhou \\
 Due on Wednesday, Mar 17 11:59 PM Eastern Time. 
\end{center}

\section{Linear Algebra III}

\begin{enumerate}

\item (10 points) Let $A \in \IR^{m \times n}$ be a matrix of rank $n$. Prove that $\| A(A^T A)^{-1} A^T \|_2 = 1$. \\
The following can be proven via an SVD break down of the $A$, the applying the definition of the $l_2$ norm to the 
derivation. Note that in the second step, we say that $A^TA = B$, in which $B$ is a square matrix. We can then say
that, by definition, $B^{-1} = V \Sigma^{-1} U^T$. In this case, $B = V \Sigma^2 V^T \Rightarrow B^{-1} = V \Sigma^{-2} V^T$.
\begin{align*}
    A(A^T A)^{-1} A^T &= U \Sigma V^T (V \Sigma^2 V^T)^{-1} V \Sigma U^T\\
                      &= U \Sigma V^T V \Sigma^{-2} V^T V \Sigma U^T \\
                      &= U \Sigma \Sigma^{-2}  \Sigma U^T \\
                      &= U \begin{pmatrix} I_n, 0 \\ 0, 0 \end{pmatrix} U^T \\
\end{align*}
From there, since we know that $ \| A \|_2 = \sigma_1$ and the $\Sigma$ we found in here is the identity matrix, we can 
say that $\| A(A^T A)^{-1} A^T \|_2 = 1$, since the first singular value is 1 in the matrix.

\item (10 points) Let $A$ and $B$ be two positive semi-definite matrices in $\IR^{n \times n}$. Prove or disprove: 
\begin{enumerate}
\item $A+ B$ is positive semi-definite\\
This is true. We can prove through the definitions of what a positive semi-definite matrix is, then add them together
and show the addition is greater than or equal to 0, and therefore a positive semi-definite matrix.
\begin{align*}
    0 &\leq v^T A \\   
    0 &\leq v^T B v \\  
    0 &\leq v^T A v + v^T B v   \\
    0 &\leq v^T (A + B) v   \\
\end{align*}

\item $AB$ is positive semi-definite\\
False, since this only holds true if $AB$ is a symmetric matrix. A counter example this question shown below:
\begin{align*}
    A &= \begin{pmatrix} 1, 2 \\ 2, 5 \end{pmatrix}\\
    B &= \begin{pmatrix} 1, -1 \\ -1, 2 \end{pmatrix}\\
    AB &= \begin{pmatrix} -1, 3 \\ -3, 8 \end{pmatrix}\\
\end{align*}
$AB$ here is not positive semi-definite since it is not greater than or equal to 0 for all $v$, as shown below:
\begin{align*}
    0 &\leq v^T AB v\\
    -1 &= \begin{pmatrix} 1, 0 \end{pmatrix} \begin{pmatrix} -1, 3 \\ -3, 8 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \leq 0
\end{align*}

\item $B^T$ is positive semi-definite\\
This is true. Since we know that $0 \leq v^T B v$ and that it results in a scalar value, we can say that
$v^T B v = (v^T B v)^T = v^T B^T v$. Therefore, $0 \leq v^T B^T v$ and positive semi-definite.
\end{enumerate} 

\end{enumerate}


\section{Linear Classification} 

Questions in the textbook Pattern Recognition and Machine Learning:
\begin{enumerate}
\item (10 points) Page 220, Question 4.1\\
Start by solving for the decision boundaries $f(x)$ and $f(y)$, subbing in the values from the convex hull.
\begin{align*}
    f(x) &= \mathbf{w}^T \mathbf{x}_n + w_0
    f(x) &= \mathbf{w}^T (\Sigma_n \alpha_n x_n) + w_0
    f(x) &= \Sigma_n \alpha_n (\mathbf{w}^T  x_n + w_0)
    f(y) &= \mathbf{w}^T \mathbf{y}_n + w_0
    f(y) &= \mathbf{w}^T (\Sigma_n \alpha_n y_n) + w_0
    f(y) &= \Sigma_n \alpha_n (\mathbf{w}^T  y_n + w_0)
\end{align*}
If these two hulls intersected, they would share the same boundary. This means that instead of $f(x) > 0 $ and $f(y) < 0$,
$f(x) = 0 = f(y)$. If there was some intersecting point $z$ on the boundary, then we could say 
\begin{align*}
    f(z) &= \Sigma_n \alpha_n (\mathbf{w}^T  y_n + w_0) = \Sigma_n \alpha_n (\mathbf{w}^T  x_n + w_0)
\end{align*}
This contradicts our definition of linear separability, as a point must be either greater or less than 0. So, there cannot be an intersecting
point, otherwise linear separability is broken.

\item (10 points) Page 221, Question 4.5\\
Start with the denominator, then substitute the different values for $s_k^2$ in.
\begin{align*}
    J(\bw) &= \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}\\
    s_1^2 &= \Sigma_{n \in C_1}(\bw^Tx_n - \bw^T\bm_1)^2\\
    s_1^2 &= \Sigma_{n \in C_1}(\bw^T)^2(\bx_n - \bm_1))^2\\
    s_1^2 &= (\bw^T)^2 \Sigma_{n \in C_1}(\bx_n - \bm_1)^2\\
    s_1^2 + s_2^2 &= (\bw^T)^2 (\Sigma_{n \in C_1}(\bx_n - \bm_1)^2 + \Sigma_{n \in C_2}(\bx_n - \bm_2)^2)\\
    s_1^2 + s_2^2 &= \bw^T\mathbf{S}_{\bW}\bw \\
    \mathbf{S}_{\bW} &= \Sigma_{n \in C_1}(x_n - m_1)^2 + \Sigma_{n \in C_2}(x_n - m_2)^2\\
    \mathbf{S}_{\bW} &= \Sigma_{n \in C_1}(\bx_n - \bm_1)(\bx_n - \bm_1)^T + \Sigma_{n \in C_2}(\bx_n - m_2)(\bx_n - \bm_2)^T\\
\end{align*}
Then do the same for $m_k$ in the numerator.\\
\begin{align*}
    (m_2 - m_1)^2 &= (\bw^T\bm_2 - \bw^T\bm_1)^2\\
    (m_2 - m_1)^2 &= (\bw^T)^2(\bm_2 - \bm_1)^2\\
    (m_2 - m_1)^2 &= \bw^T\mathbf{S}_{\bB}\bw\\
    \mathbf{S}_{\bB} &= (\bm_2 - \bm_1)^2\\
    \mathbf{S}_{\bB} &= (\bm_2 - \bm_1)(\bm_2 - \bm_1)^T\\
    J(\bw) &= \frac{\bw^T\mathbf{S}_{\bB}\bw}{\bw^T\mathbf{S}_{\bW}\bw}\\
\end{align*}


\item (10 points) Page 221, Question 4.6\\
Start by splitting the equation into two pieces, one for each class. Then, combine the $t_n$ and outer $x_n$, since as we know 
from the notes they combined equal the right hand side to (4.37).
\begin{align*}
    \Sigma_{n=1}^N (\bw^T x_n + w_0 -t_n)x_n = 0 \\
    \Sigma_{n \in C_1} (\bw^T x_n + w_0 -t_n)x_n + \Sigma_{n \in C_2} (\bw^T x_n + w_0 -t_n)x_n = 0 \\
    \Sigma_{n \in C_1} (\bw^T x_n + w_0 )x_n + \Sigma_{n \in C_2} (\bw^T x_n + w_0 )x_n = N(m_1 - m_2)
\end{align*}
Then, we can distribute $x_n$ and pull out $\bw$. We can then expand $m$ and apply $x_n$ to that, noting that $\Sigma_{n \in C_1} = N_1m_1$ 
and similarly for $C_2$. For this expansion, we will focus on $C_1$, since the processes for $C_2$ is equivalent. After it's expanded, we can
complete the square and reduce the terms to get a half of $\mathbf{S}_{\bB}$ and $\mathbf{S}_{\bW}$. Once we apply that to $C_2$, we 
can combine them to get our expression.
\begin{align*}
    \Sigma_{n \in C_1} (\bw x_n^T x_n + x_n m^T \bw)  + \Sigma_{n \in C_2} (\bw x_n^T x_n + x_n m^T \bw) = N(m_1 - m_2)\\
    (\Sigma_{n \in C_1} (x_n x_n^T + x_n m^T) + \Sigma_{n \in C_2} (x_n^T x_n + x_n m^T)) \bw = N(m_1 - m_2)\\
    (\Sigma_{n \in C_1} (x_n x_n^T - x_n (\frac{N_1}{N}m_1 + \frac{N_2}{N}m_2)^T) + ...\\
    (\Sigma_{n \in C_1} (x_n x_n^T - \frac{N_1N_2}{N}m_1m_1^T - \frac{N_2N_1}{N}m_1m_2^T) + ...\\
    (\Sigma_{n \in C_1} (x_n x_n^T - 2 m_1x_n^T + m_1m_1^T) + \frac{N_2N_1}{N}(m_1m_1^T - m_1m_2^T) +...\\
    (\Sigma_{n \in C_1} (x_n - m_1)(x_n - m_1)^T + \frac{N_2N_1}{N}(m_1m_1^T - m_1m_2^T) +...\\
    (\mathbf{S}_{\bW} + \frac{N_2N_1}{N}\mathbf{S}_{\bB})\bw = N(m_1 - m_2)
\end{align*}

\item (10 points) Page 222, Question 4.15\\
As we can see from (4.97), $\bH = \Phi^T \bR \Phi$ is positive definite. First, we see that $\bR$ is built from the results of a sigmoid function $y_n$,  
which means that the values of $\bR$ will always be greater than 0. We can then also show that for an arbitrary vector $v$ $v^t \bH v \geq 0$.
\begin{align*}
    0 &< v^t \bH v\\
    0 &< v^t \Phi^T \bR \Phi v\\
    0 &< v^t \Phi \bR \Phi^T v\\
    0 &< v^t \Phi \bR (v^T \Phi)^T\\
    0 &< \| v^t \bR \Phi \|^2 \\
\end{align*}
Since both $\bR$ and $\| v^t \bR \|$ are greater than 0, we can say that $\bH$ is positive definite.

\end{enumerate}

\section{Linear Regression: Experiment} 

 (40 points) In this part of homework you will explore the ridge regression and the effects of
$\ell_2$-norm regularization. You are to implement a MATLAB solver for ridge regression:
$$ \min_w \frac{1}{2}\|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2. $$
You are not allowed to use the integrated ridge regression in MATLAB.
You will use your solver to investigate the effects of the regularization on the \textsc{Diabetes}\footnote{\url{https://github.com/jiayuzhou/CSE847/blob/master/data/diabetes.mat?raw=true}}
dataset, and study the cross validation procedure.

\begin{enumerate}
\item Implement the ridge regression solver.  
\item Train regression models on the \textsc{Diabetes} dataset using 
the training data (x\_train, y\_train variables in the data file). 
Vary the $\lambda$ from ${1e-5}, {1e-4}, {1e-3}, {1e-2}, {1e-1}, 1, 10$ (In 
Matlab $1e-1$ means $0.1$). 
Compute training error (predict y\_train given X\_train), testing error (predict y\_test given X\_test) for each $\lambda$. 
The error is measured by mean squared error (MSE):
$$
\mbox{MSE} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat y_i)^2,
$$
where $N$ is the number of samples on which the error is computed, $y_i$ 
is ground truth, and $\hat y_i$ is the prediction from data points 
given model $w$. 
\item Perform 5-fold cross validation on the training data to estimate the best $\lambda$ from training data.
\end{enumerate}

In the homework, attach a brief report. In the report you need to
discuss your findings in the experiment, include a plot showing how
training/testing error changes when you vary the parameter $\lambda$ (use log
scale on $\lambda$). In the same plot, show the best $\lambda$ obtained from
your 5-fold cross validation procedure. Submit the MATLAB code (do add some comments in your code for others to understand your code) to the D2L along with your report. 

\end{document}
